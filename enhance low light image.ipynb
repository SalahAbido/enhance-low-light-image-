{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install keras\n! pip install huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nfrom huggingface_hub import from_pretrained_keras # download the model\nimport keras # deep learning\nimport os\nfrom PIL import Image, ImageOps # Image processing\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images'):\n  #  for filename in filenames:\n      #  print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T11:45:04.868063Z","iopub.execute_input":"2022-05-17T11:45:04.868403Z","iopub.status.idle":"2022-05-17T11:45:11.501275Z","shell.execute_reply.started":"2022-05-17T11:45:04.868312Z","shell.execute_reply":"2022-05-17T11:45:11.500468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = from_pretrained_keras(\"keras-io/lowlight-enhance-mirnet\", compile=False)# you need to free this from memory becase it fill more than 5 GiGa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = \"enhanced_traininglowlight\"\n  # Parent Directory path\nparent_dir = \"./\"\n  # Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\nprint(\"Directory '% s' created\" % directory)\ndirectory = \"enhanced_testinglowlight\"\n  # Parent Directory path\nparent_dir = \"./\"\n  # Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)\nprint(\"Directory '% s' created\" % directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainImageLabels = pd.read_csv('../input/cv-2022-project-scene-understanding/TrainImageLabels.csv')\nTestImageLabels = pd.read_csv('../input/cv-2022-project-scene-understanding/TestImageLabels.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:45:11.503126Z","iopub.execute_input":"2022-05-17T11:45:11.503381Z","iopub.status.idle":"2022-05-17T11:45:11.52817Z","shell.execute_reply.started":"2022-05-17T11:45:11.503345Z","shell.execute_reply":"2022-05-17T11:45:11.527543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_name =[]\n# images=[]\n# c=0 \n# enhanced_trainingimages=[]\n# training_labels=np.array([])\n# img_height = []\n# img_width = []\n# img_channel =[] \n# zipObj = ZipFile('trainingsample.zip', mode='w')\ndir_path = '../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images'\nfor file_name in  tqdm(os.listdir(dir_path)):\n#     if c<2500 :\n#         c=c+1\n#     #     start of labels\n#         for i in range(len(TrainImageLabels)):\n#             if file_name == TrainImageLabels[\"ImageName\"][i]:\n#                 training_labels=np.append(training_labels,TrainImageLabels[\"In/Out\"][i])\n#     #      end of labels        \n    #     img_name.append(file_name)\n    image = cv2.imread(dir_path+'/'+file_name) # reading image \n    image = Image.fromarray(image)\n    #     images.append(low_light_img)\n\n#     low_light_img = Image.open(dir_path+'/'+file_name)\n#     start of model\n    image = image.resize((256,256),Image.NEAREST)\n    image = keras.preprocessing.image.img_to_array(image)\n    image = image.astype('float32') / 255.0\n    image = np.expand_dims(image, axis = 0)\n    image = model.predict(image) # model inference to enhance the low light pics\n    image = image[0] * 255.0\n    image = image.clip(0,255)\n    image = image.reshape((np.shape(image)[0],np.shape(image)[1],3))\n    image = np.uint32(image)\n    image =Image.fromarray(image.astype('uint8'),'RGB')\n#     end of model \n    image.save('./enhanced_traininglowlight'+'/'+file_name, format=\"png\")\n    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n#         enhanced_trainingimages.append(image)\n#img_height.append(img.shape[1])\n#img_width.append(img.shape[0])\n#img_channel.append(img.shape[2])\n#x = int(img.shape[0]/2)\n#y = int(img.shape[1]/2)\n\n        # the problem here  is the gray scalle images try to handle this problem\n\n    #     zipObj.write('./enhanced_traininglowlight'+'/'+file_name)\n\n#     else :\n#         break  \n        # close the Zip File\n# zipObj.close()\ndel image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_name =[]\n# images=[]\n# c=0 \n# enhanced_trainingimages=[]\n# training_labels=np.array([])\n# img_height = []\n# img_width = []\n# img_channel =[] \n# zipObj = ZipFile('trainingsample.zip', mode='w')\ndir_path = '../input/cv-2022-project-scene-understanding/Test/Test/Images'\nfor file_name in  tqdm(os.listdir(dir_path)):\n    image = cv2.imread(dir_path+'/'+file_name) # reading image \n    image = Image.fromarray(image)\n    #     images.append(low_light_img)\n\n#     low_light_img = Image.open(dir_path+'/'+file_name)\n#     start of model\n    image = image.resize((256,256),Image.NEAREST)\n    image = keras.preprocessing.image.img_to_array(image)\n    image = image.astype('float32') / 255.0\n    image = np.expand_dims(image, axis = 0)\n    image = model.predict(image) # model inference to enhance the low light pics\n    image = image[0] * 255.0\n    image = image.clip(0,255)\n    image = image.reshape((np.shape(image)[0],np.shape(image)[1],3))\n    image = np.uint32(image)\n    image =Image.fromarray(image.astype('uint8'),'RGB')\n#     end of model \n    image.save('./enhanced_testinglowlight'+'/'+file_name, format=\"png\")\n    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n#         enhanced_trainingimages.append(image)\n#img_height.append(img.shape[1])\n#img_width.append(img.shape[0])\n#img_channel.append(img.shape[2])\n#x = int(img.shape[0]/2)\n#y = int(img.shape[1]/2)\n\n        # the problem here  is the gray scalle images try to handle this problem\n\n    #     zipObj.write('./enhanced_traininglowlight'+'/'+file_name)\n\n#     else :\n#         break  \n        # close the Zip File\n# zipObj.close()\ndel image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a zip file and put enhanced  training  image data in it  \nzipObj = ZipFile('trainingsample.zip', mode='w')\ndir_path = './enhanced_traininglowlight'\nfor file_name in  tqdm(os.listdir(dir_path)):\n    zipObj.write('./enhanced_traininglowlight'+'/'+file_name)\nzipObj.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T09:58:17.447425Z","iopub.execute_input":"2022-05-17T09:58:17.448036Z","iopub.status.idle":"2022-05-17T09:58:18.64611Z","shell.execute_reply.started":"2022-05-17T09:58:17.447994Z","shell.execute_reply":"2022-05-17T09:58:18.644406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a zip file and put enhanced  testing  image data in it  \nzipObj = ZipFile('testingsample.zip', mode='w')\ndir_path = './enhanced_testinglowlight'\nfor file_name in  tqdm(os.listdir(dir_path)):\n    zipObj.write('./enhanced_testinglowlight'+'/'+file_name)\nzipObj.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T09:58:20.781718Z","iopub.execute_input":"2022-05-17T09:58:20.782045Z","iopub.status.idle":"2022-05-17T09:58:21.596073Z","shell.execute_reply.started":"2022-05-17T09:58:20.78201Z","shell.execute_reply":"2022-05-17T09:58:21.595321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  reading enhanced_training data from output after enhancement \ntraining_labels=[]\nenhanced_trainingimages=[]\ndir_path = './enhanced_traininglowlight'\nfor file_name in tqdm(os.listdir(dir_path)):\n    image = cv2.imread(dir_path+'/'+file_name) # reading image \n#     output_image = Image.fromarray(output_image)\n    enhanced_trainingimages.append(image/ 255.0)\n    #     start of labels\n    for i in range(len(TrainImageLabels)):\n        if file_name == TrainImageLabels[\"ImageName\"][i]:\n            training_labels.append(TrainImageLabels[\"In/Out\"][i])\n#      end of labels","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:45:31.974378Z","iopub.execute_input":"2022-05-17T11:45:31.975153Z","iopub.status.idle":"2022-05-17T11:46:45.620379Z","shell.execute_reply.started":"2022-05-17T11:45:31.97511Z","shell.execute_reply":"2022-05-17T11:46:45.619612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  reading testing  from output after enhancement \ntesting_labels=[]\nenhanced_testingimages=[]\ndir_path = './enhanced_testinglowlight'\nfor file_name in tqdm(os.listdir(dir_path)):\n    image = cv2.imread(dir_path+'/'+file_name) # reading image \n#     output_image = Image.fromarray(output_image)\n    enhanced_testingimages.append(image/ 255.0)\n    #     start of labels\n    for i in range(len(TestImageLabels)):\n        if file_name == TestImageLabels[\"ImageName\"][i]:\n            testing_labels.append(TestImageLabels[\"In/Out\"][i])\n#      end of labels","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:46:45.622169Z","iopub.execute_input":"2022-05-17T11:46:45.622648Z","iopub.status.idle":"2022-05-17T11:47:14.693346Z","shell.execute_reply.started":"2022-05-17T11:46:45.622603Z","shell.execute_reply":"2022-05-17T11:47:14.692546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.applications.vgg16 import VGG16\n\n# # include top should be False to remove the softmax layer\n# pretrained_model = VGG16(include_top=False, weights='imagenet')\n# pretrained_model.summary()\nnp.shape(enhanced_testingimages)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:39:39.205322Z","iopub.execute_input":"2022-05-17T11:39:39.205684Z","iopub.status.idle":"2022-05-17T11:39:40.046644Z","shell.execute_reply.started":"2022-05-17T11:39:39.205648Z","shell.execute_reply":"2022-05-17T11:39:40.045887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.utils import to_categorical# extract train and val features\n# vgg_features_train = pretrained_model.predict(enhanced_trainingimages)\ndel TestImageLabels\ndel TrainImageLabels","metadata":{"execution":{"iopub.status.busy":"2022-05-17T10:36:14.871513Z","iopub.execute_input":"2022-05-17T10:36:14.872126Z","iopub.status.idle":"2022-05-17T10:36:14.877706Z","shell.execute_reply.started":"2022-05-17T10:36:14.872077Z","shell.execute_reply":"2022-05-17T10:36:14.876823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nimg_height = 256\nimg_width = 256\nclasses = np.array([\"indoors\", \"outdoors\"])\nnum_classes = len(classes)\ntraining_labels=np.array(training_labels)\n# training_labels=training_labels.reshape(3000,1)\ntesting_labels=np.array(testing_labels)\n# testing_labels=testing_labels.reshape(1800 ,1)\n\nenhanced_trainingimages=np.array(enhanced_trainingimages)\n# enhanced_trainingimages = enhanced_trainingimages / 255\n\nenhanced_testingimages=np.array(enhanced_testingimages)\n# enhanced_testingimages = enhanced_testingimages / 255.0\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:50:25.06551Z","iopub.execute_input":"2022-05-17T11:50:25.066294Z","iopub.status.idle":"2022-05-17T11:50:27.317769Z","shell.execute_reply.started":"2022-05-17T11:50:25.06625Z","shell.execute_reply":"2022-05-17T11:50:27.316958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = models.Sequential([\n    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(256, 256, 3)),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Flatten(),\n    \n    layers.Dense(64, activation='relu'),\n    layers.Dense(2, activation='softmax')\n])\ncnn.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# cnn.fit(enhanced_trainingimages, training_labels, epochs=5, batch_size =batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:50:50.751004Z","iopub.execute_input":"2022-05-17T11:50:50.751295Z","iopub.status.idle":"2022-05-17T11:50:56.585855Z","shell.execute_reply.started":"2022-05-17T11:50:50.751267Z","shell.execute_reply":"2022-05-17T11:50:56.585079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.fit(enhanced_trainingimages, training_labels, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T11:50:56.587458Z","iopub.execute_input":"2022-05-17T11:50:56.588102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = np.array([\"indoors\", \"outdoors\"])\n\nenhanced_images = np.array(enhanced_images )\nfor i in range(len(enhanced_images)):\n    enhanced_images[i] = cv2.cvtColor(np.array(enhanced_images[i]), cv2.COLOR_RGB2BGR)\nopencvImage = opencvImage / 255\nopencvImage.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating list have the name of image and the labels (indoor or outdoor)\n# trianing_image=np.array([])\nimg_name =np.array([])\ntraining_labels=np.array([])\ndir_path = '../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images'\nfor file_name in os.listdir(dir_path):\n    img_name = np.append(img_name,file_name)\n    low_light_img = cv2.imread(dir_path+'/'+file_name) \n#     trianing_image=np.append([trianing_image,low_light_img])\n    for i in range(len(TrainImageLabels)):\n        if file_name == TrainImageLabels[\"ImageName\"][i]:\n            training_labels=np.append(training_labels,TrainImageLabels[\"In/Out\"][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images/2019_00002.png')\nimage = Image.fromarray(image)\nimage.save('.//2019_00002.png', format=\"png\")\n\nzipObj = ZipFile('trainingsample.zip', 'w',)\nzipObj.write(\".//2019_00002.png\")\n\n# close the Zip File\nzipObj.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images/2019_00001.png')\nprint(image.shape)\nimage = Image.fromarray(image)\nimage = image.resize((256,256),Image.NEAREST)\nimage = keras.preprocessing.image.img_to_array(image)\nimage = image.astype('float32') / 255.0\nimage = np.expand_dims(image, axis = 0)\noutput = model.predict(image) # model inference to enhance the low light pics\noutput_image = output[0] * 255.0\noutput_image = output_image.clip(0,255)\noutput_image = output_image.reshape((np.shape(output_image)[0],np.shape(output_image)[1],3))\noutput_image = np.uint32(output_image)\noutput_image =Image.fromarray(output_image.astype('uint8'),'RGB')\na=output_image\noutput_image.save(\".//converted.png\", format=\"png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_light_img=Image.open(\"../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images/2019_00001.png\")#.convert('L')\n\nplt.imshow(low_light_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = '../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images'\nfor file_name ,i in zip(os.listdir(dir_path),range(len(TrainImageLabels))):\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=[]\ndir_path = '../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images'\nfor file_name in os.listdir(dir_path):\n    img_name.append(file_name)\n    if file_name ==\n    low_light_img = Image.open(dir_path+'/'+file_name)\n    i.append(low_light_img)\n#low_light_img = Image.open('../input/cv-2022-project-scene-understanding/Train-20220409T231819Z-001/Train/Images/2019_00001.png').convert('RGB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lis =[]\nc=0 \ndir_path = './enhanced_traininglowlight'\nfor file_name in tqdm(os.listdir(dir_path)):\n     if c<1500 :\n            c=c+1\n            lis.append(file_name )    \nprint(len(lis))\nlis =[]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_light_img = low_light_img.resize((256,256),Image.NEAREST)\nimage = keras.preprocessing.image.img_to_array(low_light_img)\nimage = image.astype('float32') / 255.0\nimage = np.expand_dims(image, axis = 0)\noutput = model.predict(image) # model inference to enhance the low light pics\noutput_image = output[0] * 255.0\noutput_image = output_image.clip(0,255)\noutput_image = output_image.reshape((np.shape(output_image)[0],np.shape(output_image)[1],3))\noutput_image = np.uint32(output_image)\nim =Image.fromarray(output_image.astype('uint8'),'RGB')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(training_labels)  # the problem here  is the gray scalle images try to handle this problem\n# training_labels.to_csv('/kaggle/working/training_labels.csv',index=False)\npd.DataFrame(training_labels).to_csv(\"path/to/file.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_light_img = cv2.imread(\"./low_light_img\") # reading image \nlow_light_img = Image.fromarray(low_light_img)\nlow_light_img\nlow_light_img.save(\"./lowlight/low_light_img.png\", format=\"png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(enhanced_images)\ndf.to_csv('enhanced_images.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = \"./\"\nos.chdir(directory)\nfilename = 'savedImage.jpg'\n  \n# Using cv2.imwrite() method\n# Saving the image\ncv2.imwrite(filename, enhanced_images[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}